1 ssh设置
检查是否能互连
ssh master
ssh slave1
ssh slave2

2 vim /etc/hosts

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.5.111 master
192.168.5.124 slave1
192.168.5.139 slave2

hostname master

3 HADOOP
#hadoop-env.sh
export JAVA_HOME=/usr/local/jdk

#vim core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
    </property>
</configuration>


#hdfs-site.xml
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/usr/local/hadoop/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/usr/local/hadoop/datanode</value>
    </property>

    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>

    <property>
        <name>dfs.namenode.secondary.http.address</name>
        <value>master:9870</value>
    </property>

    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
</configuration>


#mapred-site.xml<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
         <name>yarn.app.mapreduce.am.env</name>
         <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
         <name>mapreduce.map.env</name>
         <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
         <name>mapreduce.reduce.env</name>
         <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>

    <property>
          <name>mapred.job.tracker</name>
          <value>master:9001</value>
    </property>

    <property>
　　     <name>mapreduce.map.memory.mb</name>
　　     <value>1536</value>
    </property>
    <property>
　　     <name>mapreduce.map.java.opts</name>
　　     <value>-Xmx1024M</value>
    </property>
    <property>
　　     <name>mapreduce.reduce.memory.mb</name>
　　     <value>3072</value>
    </property>
    <property>
　　     <name>mapreduce.reduce.java.opts</name>
　　     <value>-Xmx2560M</value>
    </property>

    <property>
         <name>mapreduce.jobhistory.address</name>
         <value>master:10020</value>
    </property>
    <property>
         <name>mapreduce.jobhistory.webapp.address</name>
         <value>master:19888</value>
    </property>
</configuration>


#vim yarn-site.xml
<configuration>
<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>



#workers
master
slave1
slave2

start-dfs.sh 和 stop-dfs.sh
export HDFS_DATANODE_USER=root
export HDFS_DATANODE_SECURE_USER=hdfs
export HDFS_NAMENODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root

start-yarn.sh 和 stop-yarn.sh
export YARN_RESOURCEMANAGER_USER=root
export HADOOP_SECURE_DN_USER=yarn
export YARN_NODEMANAGER_USER=root

4
scp -r /usr/local/hadoop slave1:/usr/local/
hdfs namenode -format
start-all.sh


###########################################################################################################


SPARK
spark-env.sh

export SCALA_HOME=/usr/local/scala
export JAVA_HOME=/usr/local/jdk
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export SPARK_MASTER_IP=master
export SPARK_LOCAL_IP=localhost
export SPAPK_LOCAL_DIRS=/usr/local/spark
export SPARK_DRIVER_MEMORY=1G

#vim slaves
master
slave1
slave2


WARN  Utils:66 - Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.5.200 instead (on interface ens33)
WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
-->
hadoop-env.sh增加以下内容
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_COMMON_LIB_NATIVE_DIR"
spark-env.sh增加以下内容
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
