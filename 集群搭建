1 ssh设置
检查是否能互连
ssh master
ssh slave1
ssh slave2

2 vim /etc/hosts

127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.5.111 master
192.168.5.124 slave1
192.168.5.139 slave2

hostname master

3 
#hadoop-env.sh
export JAVA_HOME=/usr/local/jdk

#vim core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
    </property>
</configuration>


#hdfs-site.xml
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/usr/local/hadoop/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/usr/local/hadoop/datanode</value>
    </property>

    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>

    <property>
        <name>dfs.namenode.secondary.http.address</name>
        <value>master:9870</value>
    </property>

    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
</configuration>


#mapred-site.xml<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
         <name>yarn.app.mapreduce.am.env</name>
         <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
         <name>mapreduce.map.env</name>
         <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>
    <property>
         <name>mapreduce.reduce.env</name>
         <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
    </property>

    <property>
          <name>mapred.job.tracker</name>
          <value>master:9001</value>
    </property>

    <property>
　　     <name>mapreduce.map.memory.mb</name>
　　     <value>1536</value>
    </property>
    <property>
　　     <name>mapreduce.map.java.opts</name>
　　     <value>-Xmx1024M</value>
    </property>
    <property>
　　     <name>mapreduce.reduce.memory.mb</name>
　　     <value>3072</value>
    </property>
    <property>
　　     <name>mapreduce.reduce.java.opts</name>
　　     <value>-Xmx2560M</value>
    </property>

    <property>
         <name>mapreduce.jobhistory.address</name>
         <value>master:10020</value>
    </property>
    <property>
         <name>mapreduce.jobhistory.webapp.address</name>
         <value>master:19888</value>
    </property>
</configuration>


#vim yarn-site.xml
<configuration>
<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>



#workers
master
slave1
slave2

start-dfs.sh 和 stop-dfs.sh
export HDFS_DATANODE_USER=root
export HDFS_DATANODE_SECURE_USER=hdfs
export HDFS_NAMENODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root

start-yarn.sh 和 stop-yarn.sh
export YARN_RESOURCEMANAGER_USER=root
export HADOOP_SECURE_DN_USER=yarn
export YARN_NODEMANAGER_USER=root

4
scp -r /usr/local/hadoop slave1:/usr/local/
hdfs namenode -format
start-all.sh
